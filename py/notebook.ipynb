{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a23977e",
   "metadata": {},
   "source": [
    "# Attention Is All You Need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4644ad",
   "metadata": {},
   "source": [
    "Let's try to reproduce the code described in the seminal paper\n",
    "[Attention Is All You Need](https://arxiv.org/abs/1706.03762) (which introduced the Transformer\n",
    "architecture) and write ourselves a German-English translator!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f7a01d",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Attention Is All You Need was a contribution to the [Conference on Neural Information Processing\n",
    "Systems](https://neurips.cc/Conferences/2017) (NIPS, called NeurIPS since 2018) in 2017 and used\n",
    "data of the [machine translation task](https://www.statmt.org/wmt14/translation-task.html) of the\n",
    "2014 Workshop on Statistical Machine Translation (WMT14). The dataset for the German-English\n",
    "language pair consists of three parallel corpora (collections of sentences/text snippets in multiple\n",
    "languags):\n",
    "\n",
    "* The European Parliament Proceedings Parallel Corpus (Europarl) v7. Documents produced by the\n",
    "European Parlament are usually translated into all 24 EU languages. Europarl is a collection of\n",
    "sentences in 11 languages taken from the proceedings of the European Parlament (ie. political\n",
    "speeches).\n",
    "* A parallel text corpus extracted from Common Crawl.\n",
    "* News Commentary, mostly text taken from economic and political news articles.\n",
    "\n",
    "The WMT14 task description page also lists the test set, \"newstest2014\" as well as the recommended\n",
    "dev/validation set \"newstest2013\" (the test set of WMT13)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57913502",
   "metadata": {},
   "source": [
    "Our first step is to download the needed archives from the website and extract the relevant\n",
    "German-English files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8609bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../sh/download_input_files.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0393a52f",
   "metadata": {},
   "source": [
    "\n",
    "The News Commentary files have different line break sequences, so we homogenize them using a script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722921fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run fix_line_breaks.py \\\n",
    "     ../1_input/news-commentary-v9.de-en.en ../1_input/processed/news-commentary-v9.de-en.en\n",
    "%run fix_line_breaks.py \\\n",
    "     ../1_input/news-commentary-v9.de-en.de ../1_input/processed/news-commentary-v9.de-en.de"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c6c327-4114-4830-a778-bbf7a94fd0c8",
   "metadata": {},
   "source": [
    "The newstest2014 files are in SGML. We'll extract the data into simple text files as well. There's a\n",
    "slight issue with those files, they contain ampersands which have a special meaning in XML files\n",
    "(SGML is XML), so we need to escape them before converting the files (they'll turn back into normal\n",
    "ampersands during conversion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d7c135-4134-418a-aae1-1609d35febc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run escape_ampersands.py \\\n",
    "     ../1_input/newstest2014-deen-ref.de.sgm ../1_input/processed/newstest2014.de.sgm\n",
    "%run escape_ampersands.py \\\n",
    "     ../1_input/newstest2014-deen-ref.en.sgm ../1_input/processed/newstest2014.en.sgm\n",
    "%run convert_sgm.py \\\n",
    "     ../1_input/processed/newstest2014.de.sgm ../1_input/processed/newstest2014.en.sgm \\\n",
    "     ../1_input/processed/newstest2014.de ../1_input/processed/newstest2014.en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca9eea2",
   "metadata": {},
   "source": [
    "We now have about 4.5M sentences in German and English for training, and 3K each for validation and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12ce902-d976-49cb-8fd9-2e168e3b0466",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../1_input && \\\n",
    "wc -l europarl-v7.de-en.de commoncrawl.de-en.de processed/news-commentary-v9.de-en.de && echo && \\\n",
    "wc -l europarl-v7.de-en.en commoncrawl.de-en.en processed/news-commentary-v9.de-en.en && echo && \\\n",
    "wc -l newstest2013.de && \\\n",
    "wc -l newstest2013.en && echo && \\\n",
    "wc -l processed/newstest2014.de && \\\n",
    "wc -l processed/newstest2014.en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461e15e7",
   "metadata": {},
   "source": [
    "## Cleaning the data\n",
    "\n",
    "To make things easier for ourselves and the model, we will clean the data a bit.\n",
    "\n",
    "In the test set, there is an accent character that seems to be where a space should be. We deal with\n",
    "this \"manually\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a18897",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run fix_testset.py ../1_input/processed/newstest2014.de ../tmp/newstest2014.de"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e753cffe-54b3-4c02-a969-40d2b28055fd",
   "metadata": {},
   "source": [
    "Now we apply a cleaning script to all the input file pairs to do the following:\n",
    "\n",
    "* NFKC Unicode normalization\n",
    "* Replacing some characters, e.g. all sorts of quotation marks with simple double quotes\n",
    "* Removing all pairs where one of the sentences is empty (no translation)\n",
    "* Removing all pairs where at least one of the sentences contains characters not included in a\n",
    "rather small set of standard European language characters\n",
    "\n",
    "Removing here will mean setting the line to an empty string in both files (to keep line numbers\n",
    "consistent with the original input files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be1d079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from pathlib import Path\n",
    "\n",
    "from clean import clean\n",
    "\n",
    "num_processes = multiprocessing.cpu_count()\n",
    "src_base_path = Path(\"../1_input\")\n",
    "dst_base_path = Path(\"../2_clean\")\n",
    "for de_path, en_path in [\n",
    "    [\"europarl-v7.de-en.de\", \"europarl-v7.de-en.en\"],\n",
    "    [\"commoncrawl.de-en.de\", \"commoncrawl.de-en.en\"],\n",
    "    [\"processed/news-commentary-v9.de-en.de\", \"processed/news-commentary-v9.de-en.en\"],\n",
    "    [\"newstest2013.de\", \"newstest2013.en\"],\n",
    "    [\"../tmp/newstest2014.de\", \"processed/newstest2014.en\"],\n",
    "]:\n",
    "    de_src_path = src_base_path / de_path\n",
    "    de_dst_path = dst_base_path / de_src_path.name\n",
    "    en_src_path = src_base_path / en_path\n",
    "    en_dst_path = dst_base_path / en_src_path.name\n",
    "    clean(str(de_src_path), str(en_src_path), str(de_dst_path), str(en_dst_path), num_processes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4b4d42-119a-4221-9ffe-3869c52d57db",
   "metadata": {},
   "source": [
    "The cleaned text files now still have the same number of lines as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbf2d19-5fc6-4745-871e-dfedd4d32d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../2_clean && \\\n",
    "wc -l europarl-v7.de-en.de commoncrawl.de-en.de news-commentary-v9.de-en.de && echo && \\\n",
    "wc -l europarl-v7.de-en.en commoncrawl.de-en.en news-commentary-v9.de-en.en && echo && \\\n",
    "wc -l newstest2013.de && \\\n",
    "wc -l newstest2013.en && echo && \\\n",
    "wc -l newstest2014.de && \\\n",
    "wc -l newstest2014.en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393947df-42ad-470d-a12a-ad01d9dafe62",
   "metadata": {},
   "source": [
    "However, when we count only non-empty lines we see that we lost around 70K (1.5%) of sentence pairs\n",
    "in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eebc3e-ba1c-4d74-beb4-dbbae906aac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../2_clean && \\\n",
    "../sh/count_non_empty_lines.sh \\\n",
    "    europarl-v7.de-en.de commoncrawl.de-en.de news-commentary-v9.de-en.de && echo && \\\n",
    "../sh/count_non_empty_lines.sh \\\n",
    "    europarl-v7.de-en.en commoncrawl.de-en.en news-commentary-v9.de-en.en && echo && \\\n",
    "../sh/count_non_empty_lines.sh newstest2013.de && \\\n",
    "../sh/count_non_empty_lines.sh newstest2013.en && echo && \\\n",
    "../sh/count_non_empty_lines.sh newstest2014.de && \\\n",
    "../sh/count_non_empty_lines.sh newstest2014.en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e4ce85-8477-4657-add3-287dbf4e4b4e",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Now we can create a tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84f8ecb-b71f-42cd-a65c-1abd67b77d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_helpers import run_subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a36641-9946-45ec-8ea9-b54d35221b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = [str(f) for f in Path(\"../2_clean\").iterdir() if f.is_file()]\n",
    "run_subprocess([\"python\", \"create_tokenizer.py\", \"../3_tokenizer/tokenizer.json\"] + input_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c4f79a-8b51-4d79-a90b-72d403c46daf",
   "metadata": {},
   "source": [
    "... and use it to tokenize our dataset(s). This will also remove any sequences longer than a\n",
    "specified maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca546a7-3f79-4fa1-886d-34a91edc2291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset_subproc(corpus_id: int, src: str, tgt: str, output: str) -> None:\n",
    "    run_subprocess(\n",
    "        [\n",
    "            \"python\",\n",
    "            \"tokenize_dataset.py\",\n",
    "            \"../3_tokenizer/tokenizer.json\",\n",
    "            output,\n",
    "            str(corpus_id),\n",
    "            src,\n",
    "            tgt,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "tokenize_dataset_subproc(\n",
    "    1, \"../2_clean/europarl-v7.de-en.de\", \"../2_clean/europarl-v7.de-en.en\", \"../tmp/europarl\"\n",
    ")\n",
    "tokenize_dataset_subproc(\n",
    "    2, \"../2_clean/commoncrawl.de-en.de\", \"../2_clean/commoncrawl.de-en.en\", \"../tmp/commoncrawl\"\n",
    ")\n",
    "tokenize_dataset_subproc(\n",
    "    3,\n",
    "    \"../2_clean/news-commentary-v9.de-en.de\",\n",
    "    \"../2_clean/news-commentary-v9.de-en.en\",\n",
    "    \"../tmp/news-commentary\",\n",
    ")\n",
    "run_subprocess(\n",
    "    [\n",
    "        \"python\",\n",
    "        \"concatenate_datasets.py\",\n",
    "        \"../4_tokens/train\",\n",
    "        \"../tmp/europarl\",\n",
    "        \"../tmp/commoncrawl\",\n",
    "        \"../tmp/news-commentary\",\n",
    "    ]\n",
    ")\n",
    "tokenize_dataset_subproc(\n",
    "    4, \"../2_clean/newstest2013.de\", \"../2_clean/newstest2013.en\", \"../4_tokens/newstest2013\"\n",
    ")\n",
    "tokenize_dataset_subproc(\n",
    "    5, \"../2_clean/newstest2014.de\", \"../2_clean/newstest2014.en\", \"../4_tokens/newstest2014\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16655bc2-eb4e-4ba4-a46d-1ac64a9ea731",
   "metadata": {},
   "source": [
    "## Buckets\n",
    "\n",
    "Our model will operate on batches of sentence pairs. To limit the amount of useless computation on\n",
    "padding tokens, there will be batches with different sequence lengths filled with appropriate\n",
    "sentence pairs (like they did in in the paper: \"Sentence pairs were batched together by approximate\n",
    "sequence length\"). To facilitate this we'll put our sentence pairs in buckets depending on the\n",
    "largest sequence length of either sentence. Later, to get a batch with a given sequence length, we\n",
    "can simply sample from the appropriate bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873f9f33-1c57-4db9-b5aa-0ad18a47cd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [\"train\", \"newstest2013\", \"newstest2014\"]:\n",
    "    run_subprocess([\"python\", \"create_chunked_index.py\", \"../4_tokens/\" + dataset])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
